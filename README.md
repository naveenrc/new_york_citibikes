# New York citi bike
![image](https://user-images.githubusercontent.com/30205620/28938575-75619846-785c-11e7-993c-751d7844583c.png)
## Requirements
+ Python 3.5+
+ Keras
+ Bokeh
+ Pandas
+ Numpy
+ Selenium
+ BeautifulSoup
+ Phanomjs http://phantomjs.org/download.html
+ <a href="https://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/">Official Bash for Windows beta version (Optional)</a>
+ csvkit for bash

## Download Data
##### Ride data
+ https://s3.amazonaws.com/tripdata/index.html has multiple data sets from 2013 to present
+ More information about these datasets can be found <a href="https://www.citibikenyc.com/system-data">here</a>

##### Weather data
+ https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00094728/detail
+ Documentation is present on the left side bar of same page

##### Bike stations info
+ Run download/station_info.py to get the station info from citibike API

##### Bike stations status
+ Run download/station_status.py to get each station status info from citibike API. It provides number of bikes available, number of docks available, is the station running,etc.

##### Google Maps Distance matrix API (Multiprocessing)
Code to get the transit times from one station to other based on station latitudes and longitudes is in download/transit_time.py. Google cloud API key is required in order to get the data.

##### Automate download, extract and delete zip files
+ To automate the download, run 'download/download.py'. <br>
+ I have used selenium, BeautifulSoup, <a href="http://phantomjs.org/download.html">phantomjs</a> to download Ride data,as the web page was dynamic.

+ Run 'download/extract.py' which extracts the zip files. I have written this program because some zip files are having repeated files inside them. I have used a dictionary of file names after extracting each zip file`s children and check if the child already exists before extracting other zip file. ZipFile module provides namelist() which can be used to check files inside a zip file before extracting it.<br>
+ The dictionary is saved to a json file and used while running download.py repeatedly to avoid downloading files that are already downloaded before

+ To delete the zip files and save space run 'download/delete_zip.py'

## Cleaning through multiprocessing
##### Ride data
+ These are huge files over 6 GB. So multiprocessing speeds up the process
+ Run cleaning/ride_clean.py to start cleaning
+ Below is a screenshot of Task Manager while tasks are executed
![tasks](https://user-images.githubusercontent.com/30205620/28745103-fb24b912-743e-11e7-8b11-fd233a840519.PNG)

+ This took 8455 seconds which is __2 hours 20 minutes__ on __4 core intel i5, 4GB RAM__

##### Weather data
+ On bash run the following command to install csvkit
```Shell
$sudo apt install csvkit
```
+ To list the columns
```Shell
$csvcut -n filename.csv
```
+ To get stats of a column, say 4. Check for outliers
```Shell
$csvcut -c 4 filename.csv | csvstat
```
+ To extract needed columns, say 3,4,5
```Shell
$csvcut -c 3,4,5 filename.csv > output_file.csv
```
##### Clean transit times from Google
+ Data is duplicated in this file. So, running cleaning/clean_transit.py cleans the file.

## Analyzing the data
##### Using Bokeh models
+ analyze/nyctile.py generates a New york city base map which can be used to plot our data.
+ Data has to be converted to web mercator format which is done by analyze/web_mercator.py. It takes lat, lon as parameters and outputs web mercator points.

##### Rides vs gender, Rides vs age, Rides vs user(multiprocessing)
+ analyze/gender_rides.py, age_rides.py takes the list of files that we cleaned in the previous section and grouped by time, gender, user type and age.
+ The resulting data set is used for plotting, which can be generated by running age_plot.py, gender_plot.py, user_rides.py

##### Plot popular stations
+ For this nyctile and web_mercator files are used to generate the base plot.
+ Data needed for this is generated by popular_stations.py which takes the list of cleaned files.

##### Rides vs weather
+ Weather data in NYC from 2013 to 2017 is joined to the number of rides every day obtained in previous step.

##### Transported bikes
+ transport_bikes.py takes the list of cleaned files. Each file is grouped by day and then by bike_id . Then compare start station id of current ride to previous ride`s end station id. This is nothing but checking whether the bike has started from where it is being dropped. If not then we can infer that citi bike authorities moved it to elsewhere.
+ It can be done by (df[current] != df[previous]).shift( ).cumsum( ) which compares previous row

##### Comparing with google
+ Citi bike provided that the average speed is 7.456 miles/hour
+ Transit times obtained in download section are used to calculate the average speed across all stations. Then it is used to compare with citi bike speed.

##### Time Series Analysis
+ Analysis on seasonality, trend and residuals is included in TimeSeriesAnalysis.ipynb notebook.
+ It also has forecasting using ARIMA model, spectral analysis.

## Prediction
##### Daily rides
+ To predict daily rides estimate run prediction_models/daily_rides.py
+ To predict number of bikes to be transported to a station run prediction_models/station.py

The report for this project can be found here https://naveenrc.github.io/citibikenyc/
